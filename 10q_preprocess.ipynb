{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286ecf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest.py\n",
    "import asyncio, aiohttp, async_timeout, csv, os, re, html, hashlib, textwrap, time, random, uuid, logging\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import re\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d04b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEC_UA = \"sae-research-edgar-pipeline/1.0 (contact: airlay88@gmail.com)\"  # per SEC guidance\n",
    "MAX_CONCURRENCY = 5    # stay well under 10 rps; add jitter/backoff\n",
    "TIMEOUT = 60\n",
    "\n",
    "ITEM_HEADER_PAT = re.compile(\n",
    "    r\"(?mi)^\\s*(?:part\\s+[ivxlcdm]+\\s*,?\\s*)?item\\s+(\\d{1,2}[aA]?)\\s*[\\.\\-–—:)]\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_primary_item2_from_submission(raw_txt: str):\n",
    "    # Split into <DOCUMENT> blocks\n",
    "    blocks = re.findall(r\"<DOCUMENT>(.*?)</DOCUMENT>\", raw_txt, re.I | re.S)\n",
    "    primary = None\n",
    "    for b in blocks:\n",
    "        m = re.search(r\"<TYPE>\\s*([^\\r\\n<]+)\", b, re.I)\n",
    "        if m and m.group(1).strip().upper() in {\"10-K\",\"10-Q\",\"8-K\",\"20-F\",\"40-F\"}:\n",
    "            primary = b\n",
    "            break\n",
    "    if not primary:\n",
    "        return None\n",
    "\n",
    "    m = re.search(r\"<TEXT>(.*)\", primary, re.I | re.S)\n",
    "    content = m.group(1) if m else primary\n",
    "\n",
    "    looks_html = re.search(r\"</?(html|table|div|p|span|br)\\b\", content, re.I)\n",
    "    if looks_html:\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        for t in soup([\"script\",\"style\",\"noscript\"]): t.decompose()\n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        text = content\n",
    "\n",
    "    text = html.unescape(text).replace(\"\\xa0\",\" \")\n",
    "    text = re.sub(r\"\\r\", \"\", text)\n",
    "    text = re.sub(r\"<PAGE>\\s*\", \"\", text, flags=re.I)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    # find Item 2 sections (ignore TOC by taking longest)\n",
    "    matches = list(ITEM_HEADER_PAT.finditer(text))\n",
    "    item2s = []\n",
    "    for i, m in enumerate(matches):\n",
    "        label = m.group(1).upper()\n",
    "        if label != \"2\": continue\n",
    "        start, heading_end = m.start(), m.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        item2s.append((start, heading_end, end))\n",
    "    if not item2s:\n",
    "        return None\n",
    "\n",
    "    start, heading_end, end = max(item2s, key=lambda t: t[2]-t[0])\n",
    "    out = (text[start:heading_end] + \"\\n\\n\" + text[heading_end:end]).strip()\n",
    "    out = re.sub(r\"\\n[ \\t]+\", \"\\n\", out)\n",
    "    out = re.sub(r\"[ \\t]{2,}\", \" \", out)\n",
    "    return out\n",
    "\n",
    "async def fetch(session: aiohttp.ClientSession, url: str) -> str | None:\n",
    "    # backoff & jitter\n",
    "    delay = 1.0\n",
    "    for attempt in range(6):\n",
    "        try:\n",
    "            async with async_timeout.timeout(TIMEOUT):\n",
    "                async with session.get(url) as r:\n",
    "                    if r.status == 200:\n",
    "                        text = await r.text()\n",
    "                        return text\n",
    "                    if r.status in (403, 429, 503):\n",
    "                        await asyncio.sleep(delay + random.random())\n",
    "                        delay = min(delay * 2, 30)\n",
    "                    else:\n",
    "                        return None\n",
    "        except Exception:\n",
    "            await asyncio.sleep(delay + random.random())\n",
    "            delay = min(delay * 2, 30)\n",
    "    return None\n",
    "\n",
    "async def worker(rows, out_dir_raw: Path, out_dir_clean: Path, results):\n",
    "    async with aiohttp.ClientSession(headers={\"User-Agent\": SEC_UA}) as session:\n",
    "        for row in rows:\n",
    "            url = row[\"url\"]\n",
    "            year = row.get(\"year\") or \"\"\n",
    "            cik = row.get(\"cik\") or \"unknown\"\n",
    "            raw = await fetch(session, url)\n",
    "            if not raw:\n",
    "                results.append((row, \"download_failed\")); continue\n",
    "\n",
    "            # save raw\n",
    "            acc = os.path.basename(row[\"filename\"]).replace(\".txt\",\"\")\n",
    "            raw_path = out_dir_raw / f\"{year}/{cik}/{acc}.txt\"\n",
    "            raw_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            raw_path.write_text(raw, encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            # extract item 2\n",
    "            item2 = extract_primary_item2_from_submission(raw)\n",
    "            if not item2:\n",
    "                results.append((row, \"item2_missing\")); continue\n",
    "\n",
    "            # save clean\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            clean_path = out_dir_clean / f\"{year}/{cik}/{doc_id}.txt\"\n",
    "            clean_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            clean_path.write_text(item2, encoding=\"utf-8\")\n",
    "\n",
    "            h = hashlib.sha256(item2.encode(\"utf-8\")).hexdigest()\n",
    "            results.append((row, \"ok\", doc_id, str(raw_path), str(clean_path), h, len(item2)))\n",
    "\n",
    "            # polite pacing (stay under 5 rps average)\n",
    "            await asyncio.sleep(0.2 + random.random() * 0.2)\n",
    "\n",
    "def chunked(iterable, n):\n",
    "    buf = []\n",
    "    for x in iterable:\n",
    "        buf.append(x)\n",
    "        if len(buf) == n:\n",
    "            yield buf\n",
    "            buf = []\n",
    "    if buf:\n",
    "        yield buf\n",
    "\n",
    "def run_ingest(csv_path, out_root=\"data\"):\n",
    "    out_dir_raw = Path(out_root) / \"raw\"\n",
    "    out_dir_clean = Path(out_root) / \"clean_item2\"\n",
    "    rows = []\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        rows = list(rdr)\n",
    "\n",
    "    # simple parallelism across N workers (sequential inside each to enforce pacing)\n",
    "    results = []\n",
    "    nworkers = 4\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [worker(part, out_dir_raw, out_dir_clean, results) for part in chunked(rows, max(1, len(rows)//nworkers))]\n",
    "    loop.run_until_complete(asyncio.gather(*tasks))\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15691c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b101110'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 19:25:17,359 INFO edgar_ingest :: run_ingest: rows=25079 workers=4 max_concurrency=5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrun_ingest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/10q/index_2013.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medgar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(res)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 204\u001b[0m, in \u001b[0;36mrun_ingest\u001b[0;34m(csv_path, out_root)\u001b[0m\n\u001b[1;32m    199\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m    200\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    201\u001b[0m     worker(part, out_dir_clean, results, worker_id\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunked(rows, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(rows)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mnworkers)))\n\u001b[1;32m    203\u001b[0m ]\n\u001b[0;32m--> 204\u001b[0m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_ingest complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    628\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    587\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 19:25:17,551 INFO edgar_ingest :: [w1 1/6269 2013/1000045/0001193125-13-046001] start fetch: https://www.sec.gov/Archives/edgar/data/1000045/0001193125-13-046001.txt\n",
      "2025-09-17 19:25:17,552 INFO edgar_ingest :: [w2 1/6269 2013/1462567/0001193125-13-221116] start fetch: https://www.sec.gov/Archives/edgar/data/1462567/0001193125-13-221116.txt\n",
      "2025-09-17 19:25:17,553 INFO edgar_ingest :: [w3 1/6269 2013/1355001/0001398987-13-000140] start fetch: https://www.sec.gov/Archives/edgar/data/1355001/0001398987-13-000140.txt\n",
      "2025-09-17 19:25:17,553 INFO edgar_ingest :: [w4 1/6269 2013/1117297/0001193125-13-430442] start fetch: https://www.sec.gov/Archives/edgar/data/1117297/0001193125-13-430442.txt\n",
      "2025-09-17 19:25:17,553 INFO edgar_ingest :: [w5 1/3 2013/99359/0001564590-13-000802] start fetch: https://www.sec.gov/Archives/edgar/data/99359/0001564590-13-000802.txt\n",
      "2025-09-17 19:25:17,987 INFO edgar_ingest :: [w3 1/6269 2013/1355001/0001398987-13-000140] raw uploaded to s3://njit-sae/10q/raw/2013/1355001/0001398987-13-000140.txt\n",
      "2025-09-17 19:25:17,988 WARNING edgar_ingest :: [w3 1/6269 2013/1355001/0001398987-13-000140] item2_missing\n",
      "2025-09-17 19:25:18,304 INFO edgar_ingest :: [w1 1/6269 2013/1000045/0001193125-13-046001] raw uploaded to s3://njit-sae/10q/raw/2013/1000045/0001193125-13-046001.txt\n",
      "2025-09-17 19:25:18,527 INFO edgar_ingest :: [w1 1/6269 2013/1000045/0001193125-13-046001] cleaned saved local: edgar/clean_item2/2013/1000045/4d75e3c4-51bf-4590-8422-75ad4dc31dc5.txt\n",
      "2025-09-17 19:25:18,530 INFO edgar_ingest :: [w5 1/3 2013/99359/0001564590-13-000802] raw uploaded to s3://njit-sae/10q/raw/2013/99359/0001564590-13-000802.txt\n",
      "2025-09-17 19:25:18,721 INFO edgar_ingest :: [w5 1/3 2013/99359/0001564590-13-000802] cleaned saved local: edgar/clean_item2/2013/99359/280bd937-a41e-4acb-9e51-792c58f698bc.txt\n",
      "2025-09-17 19:25:18,722 INFO edgar_ingest :: [w3 2/6269 2013/1355001/0001398987-13-000138] start fetch: https://www.sec.gov/Archives/edgar/data/1355001/0001398987-13-000138.txt\n",
      "2025-09-17 19:25:18,725 INFO edgar_ingest :: [w1 1/6269 2013/1000045/0001193125-13-046001] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1000045/4d75e3c4-51bf-4590-8422-75ad4dc31dc5.txt\n",
      "2025-09-17 19:25:18,752 INFO edgar_ingest :: [w4 1/6269 2013/1117297/0001193125-13-430442] raw uploaded to s3://njit-sae/10q/raw/2013/1117297/0001193125-13-430442.txt\n",
      "2025-09-17 19:25:18,932 INFO edgar_ingest :: [w4 1/6269 2013/1117297/0001193125-13-430442] cleaned saved local: edgar/clean_item2/2013/1117297/92fee8fe-2494-479a-b364-dd548cf48396.txt\n",
      "2025-09-17 19:25:18,934 INFO edgar_ingest :: [w5 1/3 2013/99359/0001564590-13-000802] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/99359/280bd937-a41e-4acb-9e51-792c58f698bc.txt\n",
      "2025-09-17 19:25:18,935 INFO edgar_ingest :: [w2 1/6269 2013/1462567/0001193125-13-221116] raw uploaded to s3://njit-sae/10q/raw/2013/1462567/0001193125-13-221116.txt\n",
      "2025-09-17 19:25:19,192 INFO edgar_ingest :: [w2 1/6269 2013/1462567/0001193125-13-221116] cleaned saved local: edgar/clean_item2/2013/1462567/098d3ad1-1c95-488f-9a2c-d894467eab72.txt\n",
      "2025-09-17 19:25:19,196 INFO edgar_ingest :: [w4 1/6269 2013/1117297/0001193125-13-430442] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1117297/92fee8fe-2494-479a-b364-dd548cf48396.txt\n",
      "2025-09-17 19:25:19,197 INFO edgar_ingest :: [w1 2/6269 2013/1000230/0001193125-13-109588] start fetch: https://www.sec.gov/Archives/edgar/data/1000230/0001193125-13-109588.txt\n",
      "2025-09-17 19:25:19,198 INFO edgar_ingest :: [w5 2/3 2013/99780/0000099780-13-000136] start fetch: https://www.sec.gov/Archives/edgar/data/99780/0000099780-13-000136.txt\n",
      "2025-09-17 19:25:19,248 INFO edgar_ingest :: [w2 1/6269 2013/1462567/0001193125-13-221116] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1462567/098d3ad1-1c95-488f-9a2c-d894467eab72.txt\n",
      "2025-09-17 19:25:19,432 INFO edgar_ingest :: [w4 2/6269 2013/1117480/0001144204-13-061281] start fetch: https://www.sec.gov/Archives/edgar/data/1117480/0001144204-13-061281.txt\n",
      "2025-09-17 19:25:19,582 INFO edgar_ingest :: [w2 2/6269 2013/1462633/0001193125-13-190930] start fetch: https://www.sec.gov/Archives/edgar/data/1462633/0001193125-13-190930.txt\n",
      "2025-09-17 19:25:19,721 INFO edgar_ingest :: [w1 2/6269 2013/1000230/0001193125-13-109588] raw uploaded to s3://njit-sae/10q/raw/2013/1000230/0001193125-13-109588.txt\n",
      "2025-09-17 19:25:19,896 INFO edgar_ingest :: [w1 2/6269 2013/1000230/0001193125-13-109588] cleaned saved local: edgar/clean_item2/2013/1000230/48d4e726-2d8a-41d0-a374-98c94bd7a30e.txt\n",
      "2025-09-17 19:25:19,973 INFO edgar_ingest :: [w1 2/6269 2013/1000230/0001193125-13-109588] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1000230/48d4e726-2d8a-41d0-a374-98c94bd7a30e.txt\n",
      "2025-09-17 19:25:20,218 INFO edgar_ingest :: [w1 3/6269 2013/1001039/0001001039-13-000019] start fetch: https://www.sec.gov/Archives/edgar/data/1001039/0001001039-13-000019.txt\n",
      "2025-09-17 19:25:20,724 INFO edgar_ingest :: [w2 2/6269 2013/1462633/0001193125-13-190930] raw uploaded to s3://njit-sae/10q/raw/2013/1462633/0001193125-13-190930.txt\n",
      "2025-09-17 19:25:20,974 INFO edgar_ingest :: [w2 2/6269 2013/1462633/0001193125-13-190930] cleaned saved local: edgar/clean_item2/2013/1462633/65d46500-ca59-4906-a67b-37f42715a3ce.txt\n",
      "2025-09-17 19:25:20,980 INFO edgar_ingest :: [w5 2/3 2013/99780/0000099780-13-000136] raw uploaded to s3://njit-sae/10q/raw/2013/99780/0000099780-13-000136.txt\n",
      "2025-09-17 19:25:21,548 INFO edgar_ingest :: [w5 2/3 2013/99780/0000099780-13-000136] cleaned saved local: edgar/clean_item2/2013/99780/cf3b1566-d02a-4bc3-b459-a88c23575989.txt\n",
      "2025-09-17 19:25:21,554 INFO edgar_ingest :: [w3 2/6269 2013/1355001/0001398987-13-000138] raw uploaded to s3://njit-sae/10q/raw/2013/1355001/0001398987-13-000138.txt\n",
      "2025-09-17 19:25:21,987 INFO edgar_ingest :: [w3 2/6269 2013/1355001/0001398987-13-000138] cleaned saved local: edgar/clean_item2/2013/1355001/bdba19f2-0c16-4818-8580-b6c016ed1169.txt\n",
      "2025-09-17 19:25:21,988 INFO edgar_ingest :: [w2 2/6269 2013/1462633/0001193125-13-190930] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1462633/65d46500-ca59-4906-a67b-37f42715a3ce.txt\n",
      "2025-09-17 19:25:21,990 INFO edgar_ingest :: [w4 2/6269 2013/1117480/0001144204-13-061281] raw uploaded to s3://njit-sae/10q/raw/2013/1117480/0001144204-13-061281.txt\n",
      "2025-09-17 19:25:22,192 INFO edgar_ingest :: [w4 2/6269 2013/1117480/0001144204-13-061281] cleaned saved local: edgar/clean_item2/2013/1117480/677de5e4-6256-43d5-9587-81d72f08deaf.txt\n",
      "2025-09-17 19:25:22,198 INFO edgar_ingest :: [w5 2/3 2013/99780/0000099780-13-000136] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/99780/cf3b1566-d02a-4bc3-b459-a88c23575989.txt\n",
      "2025-09-17 19:25:22,198 INFO edgar_ingest :: [w3 2/6269 2013/1355001/0001398987-13-000138] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1355001/bdba19f2-0c16-4818-8580-b6c016ed1169.txt\n",
      "2025-09-17 19:25:22,267 INFO edgar_ingest :: [w4 2/6269 2013/1117480/0001144204-13-061281] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1117480/677de5e4-6256-43d5-9587-81d72f08deaf.txt\n",
      "2025-09-17 19:25:22,387 INFO edgar_ingest :: [w2 3/6269 2013/1462694/0000930413-13-002875] start fetch: https://www.sec.gov/Archives/edgar/data/1462694/0000930413-13-002875.txt\n",
      "2025-09-17 19:25:22,497 INFO edgar_ingest :: [w3 3/6269 2013/1355042/0001213900-13-004253] start fetch: https://www.sec.gov/Archives/edgar/data/1355042/0001213900-13-004253.txt\n",
      "2025-09-17 19:25:22,515 INFO edgar_ingest :: [w5 3/3 2013/9984/0000009984-13-000122] start fetch: https://www.sec.gov/Archives/edgar/data/9984/0000009984-13-000122.txt\n",
      "2025-09-17 19:25:22,571 INFO edgar_ingest :: [w1 3/6269 2013/1001039/0001001039-13-000019] raw uploaded to s3://njit-sae/10q/raw/2013/1001039/0001001039-13-000019.txt\n",
      "2025-09-17 19:25:22,841 INFO edgar_ingest :: [w1 3/6269 2013/1001039/0001001039-13-000019] cleaned saved local: edgar/clean_item2/2013/1001039/ff148612-0be3-4376-a9b4-13bbee664441.txt\n",
      "2025-09-17 19:25:22,854 INFO edgar_ingest :: [w4 3/6269 2013/1117733/0001193125-13-440366] start fetch: https://www.sec.gov/Archives/edgar/data/1117733/0001193125-13-440366.txt\n",
      "2025-09-17 19:25:22,906 INFO edgar_ingest :: [w1 3/6269 2013/1001039/0001001039-13-000019] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1001039/ff148612-0be3-4376-a9b4-13bbee664441.txt\n",
      "2025-09-17 19:25:23,194 INFO edgar_ingest :: [w1 4/6269 2013/1001115/0001193125-13-042120] start fetch: https://www.sec.gov/Archives/edgar/data/1001115/0001193125-13-042120.txt\n",
      "2025-09-17 19:25:23,408 INFO edgar_ingest :: [w5 3/3 2013/9984/0000009984-13-000122] raw uploaded to s3://njit-sae/10q/raw/2013/9984/0000009984-13-000122.txt\n",
      "2025-09-17 19:25:23,743 INFO edgar_ingest :: [w5 3/3 2013/9984/0000009984-13-000122] cleaned saved local: edgar/clean_item2/2013/9984/21e3bc68-96bb-45bd-8e8e-973c90cfcf45.txt\n",
      "2025-09-17 19:25:23,752 INFO edgar_ingest :: [w4 3/6269 2013/1117733/0001193125-13-440366] raw uploaded to s3://njit-sae/10q/raw/2013/1117733/0001193125-13-440366.txt\n",
      "2025-09-17 19:25:24,018 INFO edgar_ingest :: [w4 3/6269 2013/1117733/0001193125-13-440366] cleaned saved local: edgar/clean_item2/2013/1117733/1a97dab0-2273-4c53-bc0f-52fecff91f85.txt\n",
      "2025-09-17 19:25:24,022 INFO edgar_ingest :: [w2 3/6269 2013/1462694/0000930413-13-002875] raw uploaded to s3://njit-sae/10q/raw/2013/1462694/0000930413-13-002875.txt\n",
      "2025-09-17 19:25:24,186 INFO edgar_ingest :: [w2 3/6269 2013/1462694/0000930413-13-002875] cleaned saved local: edgar/clean_item2/2013/1462694/2cec78d2-b0e3-4940-9901-e755c2459637.txt\n",
      "2025-09-17 19:25:24,187 INFO edgar_ingest :: [w5 3/3 2013/9984/0000009984-13-000122] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/9984/21e3bc68-96bb-45bd-8e8e-973c90cfcf45.txt\n",
      "2025-09-17 19:25:24,189 INFO edgar_ingest :: [w3 3/6269 2013/1355042/0001213900-13-004253] raw uploaded to s3://njit-sae/10q/raw/2013/1355042/0001213900-13-004253.txt\n",
      "2025-09-17 19:25:24,517 INFO edgar_ingest :: [w3 3/6269 2013/1355042/0001213900-13-004253] cleaned saved local: edgar/clean_item2/2013/1355042/c96a0e0b-c1e4-4722-9a2f-15fb32d79384.txt\n",
      "2025-09-17 19:25:24,523 INFO edgar_ingest :: [w4 3/6269 2013/1117733/0001193125-13-440366] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1117733/1a97dab0-2273-4c53-bc0f-52fecff91f85.txt\n",
      "2025-09-17 19:25:24,524 INFO edgar_ingest :: [w2 3/6269 2013/1462694/0000930413-13-002875] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1462694/2cec78d2-b0e3-4940-9901-e755c2459637.txt\n",
      "2025-09-17 19:25:24,624 INFO edgar_ingest :: [w3 3/6269 2013/1355042/0001213900-13-004253] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1355042/c96a0e0b-c1e4-4722-9a2f-15fb32d79384.txt\n",
      "2025-09-17 19:25:24,732 INFO edgar_ingest :: [w1 4/6269 2013/1001115/0001193125-13-042120] raw uploaded to s3://njit-sae/10q/raw/2013/1001115/0001193125-13-042120.txt\n",
      "2025-09-17 19:25:24,895 INFO edgar_ingest :: [w1 4/6269 2013/1001115/0001193125-13-042120] cleaned saved local: edgar/clean_item2/2013/1001115/c32e61f9-0903-491b-9e0b-de11275ad780.txt\n",
      "2025-09-17 19:25:24,896 INFO edgar_ingest :: [w4 4/6269 2013/1118037/0001144204-13-060445] start fetch: https://www.sec.gov/Archives/edgar/data/1118037/0001144204-13-060445.txt\n",
      "2025-09-17 19:25:24,898 INFO edgar_ingest :: [w2 4/6269 2013/1463101/0001463101-13-000006] start fetch: https://www.sec.gov/Archives/edgar/data/1463101/0001463101-13-000006.txt\n",
      "2025-09-17 19:25:24,944 INFO edgar_ingest :: [w3 4/6269 2013/1355096/0001355096-13-000045] start fetch: https://www.sec.gov/Archives/edgar/data/1355096/0001355096-13-000045.txt\n",
      "2025-09-17 19:25:24,946 INFO edgar_ingest :: [w1 4/6269 2013/1001115/0001193125-13-042120] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1001115/c32e61f9-0903-491b-9e0b-de11275ad780.txt\n",
      "2025-09-17 19:25:25,214 INFO edgar_ingest :: [w1 5/6269 2013/1001250/0001104659-13-007939] start fetch: https://www.sec.gov/Archives/edgar/data/1001250/0001104659-13-007939.txt\n",
      "2025-09-17 19:25:25,386 INFO edgar_ingest :: [w2 4/6269 2013/1463101/0001463101-13-000006] raw uploaded to s3://njit-sae/10q/raw/2013/1463101/0001463101-13-000006.txt\n",
      "2025-09-17 19:25:25,508 INFO edgar_ingest :: [w2 4/6269 2013/1463101/0001463101-13-000006] cleaned saved local: edgar/clean_item2/2013/1463101/12fc55fa-7bcc-4349-a78f-010e6c7c52ae.txt\n",
      "2025-09-17 19:25:25,605 INFO edgar_ingest :: [w2 4/6269 2013/1463101/0001463101-13-000006] cleaned uploaded to s3://njit-sae/10q/clean_item2/2013/1463101/12fc55fa-7bcc-4349-a78f-010e6c7c52ae.txt\n",
      "2025-09-17 19:25:25,616 INFO edgar_ingest :: [w4 4/6269 2013/1118037/0001144204-13-060445] raw uploaded to s3://njit-sae/10q/raw/2013/1118037/0001144204-13-060445.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    res = run_ingest(\"./data/10q/index_2013.csv\", out_root=\"edgar\")\n",
    "    print(f\"done: {len(res)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- S3 config (edit these) ---\n",
    "S3_BUCKET = \"njit-sae\"\n",
    "S3_PREFIX_RAW = \"10q/raw\"               # raw only to S3\n",
    "S3_PREFIX_CLEAN = \"10q/clean_item2\"     # cleaned to local + S3\n",
    "\n",
    "# Thread pool for blocking S3 I/O\n",
    "_S3_EXEC = ThreadPoolExecutor(max_workers=8)\n",
    "_S3 = boto3.client(\"s3\")\n",
    "\n",
    "# ---------- Logging ----------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s :: %(message)s\",\n",
    ")\n",
    "log = logging.getLogger(\"edgar_ingest\")\n",
    "\n",
    "ITEM_HEADER_PAT = re.compile(\n",
    "    r\"(?mi)^\\s*(?:part\\s+[ivxlcdm]+\\s*,?\\s*)?item\\s+(\\d{1,2}[aA]?)\\s*[\\.\\-–—:)]\"\n",
    ")\n",
    "\n",
    "def extract_primary_item2_from_submission(raw_txt: str):\n",
    "    blocks = re.findall(r\"<DOCUMENT>(.*?)</DOCUMENT>\", raw_txt, re.I | re.S)\n",
    "    primary = None\n",
    "    for b in blocks:\n",
    "        m = re.search(r\"<TYPE>\\s*([^\\r\\n<]+)\", b, re.I)\n",
    "        if m and m.group(1).strip().upper() in {\"10-K\",\"10-Q\",\"8-K\",\"20-F\",\"40-F\"}:\n",
    "            primary = b\n",
    "            break\n",
    "    if not primary:\n",
    "        return None\n",
    "\n",
    "    m = re.search(r\"<TEXT>(.*)\", primary, re.I | re.S)\n",
    "    content = m.group(1) if m else primary\n",
    "\n",
    "    looks_html = re.search(r\"</?(html|table|div|p|span|br)\\b\", content, re.I)\n",
    "    if looks_html:\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        for t in soup([\"script\",\"style\",\"noscript\"]): \n",
    "            t.decompose()\n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        text = content\n",
    "\n",
    "    text = html.unescape(text).replace(\"\\xa0\",\" \")\n",
    "    text = re.sub(r\"\\r\", \"\", text)\n",
    "    text = re.sub(r\"<PAGE>\\s*\", \"\", text, flags=re.I)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    # find Item 2 sections (ignore TOC by taking longest)\n",
    "    matches = list(ITEM_HEADER_PAT.finditer(text))\n",
    "    item2s = []\n",
    "    for i, m in enumerate(matches):\n",
    "        label = m.group(1).upper()\n",
    "        if label != \"2\": \n",
    "            continue\n",
    "        start, heading_end = m.start(), m.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        item2s.append((start, heading_end, end))\n",
    "    if not item2s:\n",
    "        return None\n",
    "\n",
    "    start, heading_end, end = max(item2s, key=lambda t: t[2]-t[0])\n",
    "    out = (text[start:heading_end] + \"\\n\\n\" + text[heading_end:end]).strip()\n",
    "    out = re.sub(r\"\\n[ \\t]+\", \"\\n\", out)\n",
    "    out = re.sub(r\"[ \\t]{2,}\", \" \", out)\n",
    "    return out\n",
    "\n",
    "async def fetch(session: aiohttp.ClientSession, url: str) -> str | None:\n",
    "    delay = 1.0\n",
    "    for attempt in range(6):\n",
    "        try:\n",
    "            async with async_timeout.timeout(TIMEOUT):\n",
    "                log.debug(f\"GET {url} (attempt {attempt+1})\")\n",
    "                async with session.get(url) as r:\n",
    "                    if r.status == 200:\n",
    "                        text = await r.text()\n",
    "                        return text\n",
    "                    if r.status in (403, 429, 503):\n",
    "                        log.warning(f\"{url} -> {r.status}; backing off {delay:.1f}s\")\n",
    "                        await asyncio.sleep(delay + random.random())\n",
    "                        delay = min(delay * 2, 30)\n",
    "                    else:\n",
    "                        log.error(f\"{url} -> {r.status}; giving up\")\n",
    "                        return None\n",
    "        except Exception as e:\n",
    "            log.warning(f\"Fetch error {url}: {e}; backoff {delay:.1f}s\")\n",
    "            await asyncio.sleep(delay + random.random())\n",
    "            delay = min(delay * 2, 30)\n",
    "    return None\n",
    "\n",
    "def _s3_put_bytes_sync(bucket: str, key: str, data: bytes, content_type=\"text/plain; charset=utf-8\"):\n",
    "    _S3.put_object(Bucket=bucket, Key=key, Body=data, ContentType=content_type)\n",
    "\n",
    "async def s3_put_text(bucket: str, key: str, text: str):\n",
    "    data = text.encode(\"utf-8\")\n",
    "    loop = asyncio.get_running_loop()\n",
    "    await loop.run_in_executor(_S3_EXEC, _s3_put_bytes_sync, bucket, key, data)\n",
    "\n",
    "def s3_uri(bucket: str, key: str) -> str:\n",
    "    return f\"s3://{bucket}/{key}\"\n",
    "\n",
    "async def worker(rows, out_dir_clean: Path, results, worker_id: int):\n",
    "    async with aiohttp.ClientSession(headers={\"User-Agent\": SEC_UA}) as session:\n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            url = row[\"url\"]\n",
    "            year = (row.get(\"year\") or \"\").strip() or \"unknown_year\"\n",
    "            cik  = (row.get(\"cik\")  or \"\").strip() or \"unknown_cik\"\n",
    "            acc  = os.path.basename(row.get(\"filename\",\"acc.txt\")).replace(\".txt\",\"\")\n",
    "\n",
    "            row_id = f\"[w{worker_id} {idx}/{len(rows)} {year}/{cik}/{acc}]\"\n",
    "            log.info(f\"{row_id} start fetch: {url}\")\n",
    "            raw = await fetch(session, url)\n",
    "            if not raw:\n",
    "                log.error(f\"{row_id} download_failed\")\n",
    "                results.append((row, \"download_failed\"))\n",
    "                # gentle pace even on failures\n",
    "                await asyncio.sleep(0.2 + random.random() * 0.2)\n",
    "                continue\n",
    "\n",
    "            # --- RAW -> S3 only ---\n",
    "            raw_key = f\"{S3_PREFIX_RAW}/{year}/{cik}/{acc}.txt\"\n",
    "            try:\n",
    "                await s3_put_text(S3_BUCKET, raw_key, raw)\n",
    "                raw_s3 = s3_uri(S3_BUCKET, raw_key)\n",
    "                log.info(f\"{row_id} raw uploaded to {raw_s3}\")\n",
    "            except Exception as e:\n",
    "                log.error(f\"{row_id} raw_s3_upload_failed: {e}\")\n",
    "                results.append((row, \"raw_s3_upload_failed\"))\n",
    "                await asyncio.sleep(0.2 + random.random() * 0.2)\n",
    "                continue\n",
    "\n",
    "            # --- Extract Item 2 ---\n",
    "            item2 = extract_primary_item2_from_submission(raw)\n",
    "            if not item2:\n",
    "                log.warning(f\"{row_id} item2_missing\")\n",
    "                results.append((row, \"item2_missing\", raw_s3))\n",
    "                await asyncio.sleep(0.2 + random.random() * 0.2)\n",
    "                continue\n",
    "\n",
    "            # --- CLEANED -> local + S3 ---\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            clean_rel = f\"{year}/{cik}/{doc_id}.txt\"\n",
    "            clean_local = out_dir_clean / clean_rel\n",
    "            clean_local.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                clean_local.write_text(item2, encoding=\"utf-8\")\n",
    "                log.info(f\"{row_id} cleaned saved local: {clean_local}\")\n",
    "            except Exception as e:\n",
    "                log.error(f\"{row_id} clean_local_write_failed: {e}\")\n",
    "                results.append((row, \"clean_local_write_failed\", raw_s3))\n",
    "                await asyncio.sleep(0.2 + random.random() * 0.2)\n",
    "                continue\n",
    "\n",
    "            clean_key = f\"{S3_PREFIX_CLEAN}/{clean_rel}\"\n",
    "            try:\n",
    "                await s3_put_text(S3_BUCKET, clean_key, item2)\n",
    "                clean_s3 = s3_uri(S3_BUCKET, clean_key)\n",
    "                log.info(f\"{row_id} cleaned uploaded to {clean_s3}\")\n",
    "            except Exception as e:\n",
    "                log.error(f\"{row_id} clean_s3_upload_failed: {e}\")\n",
    "                results.append((row, \"clean_s3_upload_failed\", raw_s3, str(clean_local)))\n",
    "                await asyncio.sleep(0.2 + random.random() * 0.2)\n",
    "                continue\n",
    "\n",
    "            h = hashlib.sha256(item2.encode(\"utf-8\")).hexdigest()\n",
    "            results.append((\n",
    "                row, \"ok\", doc_id, raw_s3, str(clean_local), clean_s3, h, len(item2)\n",
    "            ))\n",
    "\n",
    "            # polite pacing (stay under ~5 rps average)\n",
    "            await asyncio.sleep(0.2 + random.random() * 0.2)\n",
    "\n",
    "def chunked(iterable, n):\n",
    "    buf = []\n",
    "    for x in iterable:\n",
    "        buf.append(x)\n",
    "        if len(buf) == n:\n",
    "            yield buf\n",
    "            buf = []\n",
    "    if buf:\n",
    "        yield buf\n",
    "\n",
    "def run_ingest(csv_path, out_root=\"data\", limit=None):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples:\n",
    "      - on success: (row, \"ok\", doc_id, raw_s3_uri, clean_local_path, clean_s3_uri, sha256, length)\n",
    "      - on common failures: (row, status, ...) with partial URIs as available\n",
    "    \"\"\"\n",
    "    out_dir_clean = Path(out_root) / \"clean_item2\"\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        rows = list(rdr)\n",
    "    \n",
    "    if limit is not None:\n",
    "        rows = rows[:limit]  # or random.sample(rows, limit)\n",
    "\n",
    "    results = []\n",
    "    nworkers = min(4, max(1, len(rows)))  # keep your original default while avoiding empty worker sets\n",
    "    log.info(f\"run_ingest: rows={len(rows)} workers={nworkers} max_concurrency={MAX_CONCURRENCY}\")\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [\n",
    "        worker(part, out_dir_clean, results, worker_id=i+1)\n",
    "        for i, part in enumerate(chunked(rows, max(1, len(rows)//nworkers)))\n",
    "    ]\n",
    "    loop.run_until_complete(asyncio.gather(*tasks))\n",
    "    log.info(\"run_ingest complete\")\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
